{"file_contents":{"README.md":{"content":"# Sample MCP Server for ChatGPT Deep Research\n\nThis is a sample Model Context Protocol (MCP) server designed to work with ChatGPT's Deep Research feature. It provides semantic search through OpenAI's Vector Store API and document retrieval capabilities, demonstrating how to build custom MCP servers that can extend ChatGPT with company-specific knowledge and tools.\n\n## Features\n\n- **Search Tool**: Semantic search using OpenAI Vector Store API\n- **Fetch Tool**: Complete document retrieval by ID with full content and metadata\n- **SSE Transport**: Server-Sent Events transport for real-time communication with ChatGPT\n- **Sample Data**: Includes 5 sample documents covering various technical topics\n- **MCP Compliance**: Follows OpenAI's MCP specification for deep research integration\n\n## Requirements\n\n- Python 3.8+\n- fastmcp (>=2.9.0)\n- uvicorn (>=0.34.3)\n- openai (Python SDK for vector store search)\n- pydantic (dependency of fastmcp)\n\n## Installation\n\n1. Install the required dependencies:\n```bash\npip install fastmcp uvicorn openai\n```\n\n2. Set up your OpenAI API key:\n```bash\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n3. Run the MCP server:\n```bash\npython main.py\n```\n\nThe server will start on `http://0.0.0.0:8000` with SSE transport enabled.\n\n## Usage\n\n### Connecting to ChatGPT Deep Research\n\n1. **Access ChatGPT Settings**: Go to [ChatGPT settings](https://chatgpt.com/#settings)\n2. **Navigate to Connectors**: Click on the \"Connectors\" tab\n3. **Add MCP Server**: Add your server URL: `http://your-domain:8000/sse/`\n4. **Test Connection**: The server should appear as available for deep research\n\n### Server Endpoints\n\n- **SSE Endpoint**: `http://0.0.0.0:8000/sse/` - Main MCP communication endpoint\n- **Health Check**: Server logs will show successful startup and tool registration\n\n### Available Tools\n\n#### Search Tool\n- **Purpose**: Find relevant documents using OpenAI Vector Store semantic search\n- **Input**: Search query string (natural language works best)\n- **Output**: List of matching documents from vector store with ID, title, and text snippet\n- **Usage**: ChatGPT will use this to find semantically relevant documents from your vector store\n- **Requirements**: Requires valid OpenAI API key and vector store access\n\n#### Fetch Tool  \n- **Purpose**: Retrieve complete document content from OpenAI Vector Store or local storage\n- **Input**: File ID from vector store (file-xxx) or local document ID\n- **Output**: Full document content with complete text and metadata\n- **Usage**: ChatGPT will use this to get complete document content for detailed analysis and citations\n- **Requirements**: Requires valid file IDs from vector store search results\n\n## Vector Store Integration\n\nThe server is configured to search vector store `vs_682552f3ab90819185d4b99adcae7a07` which contains documents like:\n- Palantir 10-Q financial reports\n- Other documents uploaded to your OpenAI vector store\n\nThe server uses only OpenAI Vector Store APIs for both search and content retrieval - no local fallback data.\n\n## Customization\n\n### Using Your Own Vector Store\n\n1. **Update Vector Store ID**: Change `VECTOR_STORE_ID` in `main.py` to your vector store ID\n2. **Upload Documents**: Use OpenAI's API to upload documents to your vector store\n2. **Document Structure**: Each document should include:\n   ```json\n   {\n     \"id\": \"unique_identifier\",\n     \"title\": \"Document Title\",\n     \"text\": \"Short description or excerpt\",\n     \"content\": \"Full document content\",\n     \"url\": \"https://optional-source-url.com\",\n     \"metadata\": {\n       \"category\": \"document_category\",\n       \"author\": \"Author Name\",\n       \"date\": \"2024-01-01\",\n       \"tags\": \"comma,separated,tags\"\n     }\n   }\n   ```\n\n3. **Restart Server**: The server loads data at startup, so restart after making changes\n\n### Modifying Search Logic\n\nThe search function in `main.py` can be customized for:\n- Different vector store IDs or multiple vector stores\n- Custom result filtering and ranking\n- Additional metadata processing from vector store results\n- Custom content snippet length and formatting\n\n## Deployment\n\n### Local Development\nThe server runs locally on port 8000 and is accessible for testing with ChatGPT.\n\n### Production Deployment\nFor production use:\n1. **Use HTTPS**: Ensure your server has SSL/TLS certificates\n2. **Authentication**: Consider adding OAuth or API key authentication\n3. **Rate Limiting**: Implement rate limiting for production traffic\n4. **Monitoring**: Add logging and monitoring for server health\n5. **Scaling**: Consider load balancing for high-traffic scenarios\n\n### Tunneling for Local Testing\nIf running locally and need external access, use tunneling tools like:\n```bash\n# Using ngrok\nngrok http 8000\n\n# Using cloudflare tunnel\ncloudflared tunnel --url http://localhost:8000\n```\n\n## Architecture\n\nThis MCP server uses:\n- **FastMCP**: Simplified MCP protocol implementation\n- **Uvicorn**: ASGI server for HTTP/SSE transport\n- **OpenAI Vector Store**: Semantic search and content retrieval through OpenAI's API\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Server won't start**: Check if port 8000 is already in use\n2. **ChatGPT can't connect**: Ensure the server URL is correct and accessible\n3. **No search results**: Verify your OpenAI API key and vector store ID are correct\n4. **Vector store errors**: Check that the vector store exists and contains documents\n5. **Tools not appearing**: Verify the server is running and MCP protocol is working\n\n### Debugging\n\n- Check server logs for detailed error messages\n- Use curl to test the SSE endpoint: `curl http://localhost:8000/sse/`\n- Test OpenAI API key: `python -c \"from openai import OpenAI; OpenAI().models.list()\"`\n- Verify vector store exists: Check OpenAI dashboard or API\n- Verify vector store contains documents and files\n\n## Contributing\n\nThis is a sample implementation. To extend it:\n1. Add more sophisticated search algorithms\n2. Implement database storage for larger datasets  \n3. Add authentication and security features\n4. Create additional MCP tools beyond search and fetch\n\n## License\n\nThis sample code is provided for educational and demonstration purposes.\n","size_bytes":6141},"main.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nSample MCP Server for ChatGPT Deep Research Integration\n\nThis server implements the Model Context Protocol (MCP) with search and fetch\ncapabilities designed to work with ChatGPT's deep research feature.\n\"\"\"\n\nimport logging\nimport os\nfrom typing import Dict, List, Any\n\nfrom fastmcp import FastMCP\nfrom openai import OpenAI\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# OpenAI configuration\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\nVECTOR_STORE_ID = os.environ.get(\"VECTOR_STORE_ID\", \"\")\n\n# Initialize OpenAI client\nopenai_client = OpenAI()\n\nserver_instructions = \"\"\"\nThis MCP server provides search and document retrieval capabilities \nfor deep research. Use the search tool to find relevant documents \nbased on keywords, then use the fetch tool to retrieve complete \ndocument content with citations.\n\"\"\"\n\n\ndef create_server():\n    \"\"\"Create and configure the MCP server with search and fetch tools.\"\"\"\n\n    # Initialize the FastMCP server\n    mcp = FastMCP(name=\"Sample Deep Research MCP Server\",\n                  instructions=server_instructions)\n\n    @mcp.tool()\n    async def search(query: str) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"\n        Search for documents using OpenAI Vector Store search.\n        \n        This tool searches through the vector store to find semantically relevant matches.\n        Returns a list of search results with basic information. Use the fetch tool to get \n        complete document content.\n        \n        Args:\n            query: Search query string. Natural language queries work best for semantic search.\n        \n        Returns:\n            Dictionary with 'results' key containing list of matching documents.\n            Each result includes id, title, text snippet, and optional URL.\n        \"\"\"\n        if not query or not query.strip():\n            return {\"results\": []}\n\n        if not openai_client:\n            logger.error(\"OpenAI client not initialized - API key missing\")\n            raise ValueError(\n                \"OpenAI API key is required for vector store search\")\n\n        # Search the vector store using OpenAI API\n        logger.info(f\"Searching {VECTOR_STORE_ID} for query: '{query}'\")\n\n        response = openai_client.vector_stores.search(\n            vector_store_id=VECTOR_STORE_ID, query=query)\n\n        results = []\n\n        # Process the vector store search results\n        if hasattr(response, 'data') and response.data:\n            for i, item in enumerate(response.data):\n                # Extract file_id, filename, and content\n                item_id = getattr(item, 'file_id', f\"vs_{i}\")\n                item_filename = getattr(item, 'filename', f\"Document {i+1}\")\n\n                # Extract text content from the content array\n                content_list = getattr(item, 'content', [])\n                text_content = \"\"\n                if content_list and len(content_list) > 0:\n                    # Get text from the first content item\n                    first_content = content_list[0]\n                    if hasattr(first_content, 'text'):\n                        text_content = first_content.text\n                    elif isinstance(first_content, dict):\n                        text_content = first_content.get('text', '')\n\n                if not text_content:\n                    text_content = \"No content available\"\n\n                # Create a snippet from content\n                text_snippet = text_content[:200] + \"...\" if len(\n                    text_content) > 200 else text_content\n\n                result = {\n                    \"id\": item_id,\n                    \"title\": item_filename,\n                    \"text\": text_snippet,\n                    \"url\":\n                    f\"https://platform.openai.com/storage/files/{item_id}\"\n                }\n\n                results.append(result)\n\n        logger.info(f\"Vector store search returned {len(results)} results\")\n        return {\"results\": results}\n\n    @mcp.tool()\n    async def fetch(id: str) -> Dict[str, Any]:\n        \"\"\"\n        Retrieve complete document content by ID for detailed \n        analysis and citation. This tool fetches the full document \n        content from OpenAI Vector Store. Use this after finding \n        relevant documents with the search tool to get complete \n        information for analysis and proper citation.\n        \n        Args:\n            id: File ID from vector store (file-xxx) or local document ID\n            \n        Returns:\n            Complete document with id, title, full text content, \n            optional URL, and metadata\n            \n        Raises:\n            ValueError: If the specified ID is not found\n        \"\"\"\n        if not id:\n            raise ValueError(\"Document ID is required\")\n\n        if not openai_client:\n            logger.error(\"OpenAI client not initialized - API key missing\")\n            raise ValueError(\n                \"OpenAI API key is required for vector store file retrieval\")\n\n        logger.info(f\"Fetching content from vector store for file ID: {id}\")\n\n        # Fetch file content from vector store\n        content_response = openai_client.vector_stores.files.content(\n            vector_store_id=VECTOR_STORE_ID, file_id=id)\n\n        # Get file metadata\n        file_info = openai_client.vector_stores.files.retrieve(\n            vector_store_id=VECTOR_STORE_ID, file_id=id)\n\n        # Extract content from paginated response\n        file_content = \"\"\n        if hasattr(content_response, 'data') and content_response.data:\n            # Combine all content chunks from FileContentResponse objects\n            content_parts = []\n            for content_item in content_response.data:\n                if hasattr(content_item, 'text'):\n                    content_parts.append(content_item.text)\n            file_content = \"\\n\".join(content_parts)\n        else:\n            file_content = \"No content available\"\n\n        # Use filename as title and create proper URL for citations\n        filename = getattr(file_info, 'filename', f\"Document {id}\")\n\n        result = {\n            \"id\": id,\n            \"title\": filename,\n            \"text\": file_content,\n            \"url\": f\"https://platform.openai.com/storage/files/{id}\",\n            \"metadata\": None\n        }\n\n        # Add metadata if available from file info\n        if hasattr(file_info, 'attributes') and file_info.attributes:\n            result[\"metadata\"] = file_info.attributes\n\n        logger.info(f\"Fetched vector store file: {id}\")\n        return result\n\n    return mcp\n\n\ndef main():\n    \"\"\"Main function to start the MCP server.\"\"\"\n    # Verify OpenAI client is initialized\n    if not openai_client:\n        logger.error(\n            \"OpenAI API key not found. Please set OPENAI_API_KEY environment variable.\"\n        )\n        raise ValueError(\"OpenAI API key is required\")\n\n    logger.info(f\"Using vector store: {VECTOR_STORE_ID}\")\n\n    # Create the MCP server\n    server = create_server()\n\n    # Configure and start the server\n    logger.info(\"Starting MCP server on 0.0.0.0:8000\")\n    logger.info(\"Server will be accessible via SSE transport\")\n\n    try:\n        # Use FastMCP's built-in run method with SSE transport\n        server.run(transport=\"sse\", host=\"0.0.0.0\", port=8000)\n    except KeyboardInterrupt:\n        logger.info(\"Server stopped by user\")\n    except Exception as e:\n        logger.error(f\"Server error: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":7493},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"fastmcp>=2.9.0\",\n    \"openai>=1.91.0\",\n    \"uvicorn>=0.34.3\",\n]\n","size_bytes":211},"replit.md":{"content":"# Replit.md\n\n## Overview\n\nThis is a sample Model Context Protocol (MCP) server designed to integrate with ChatGPT's Deep Research feature. The application provides search and document retrieval capabilities for a knowledge base through a FastMCP server implementation. It serves as a reference implementation for building custom MCP servers that can extend ChatGPT with company-specific knowledge and tools.\n\n## System Architecture\n\nThe application follows a simple server-based architecture:\n\n- **Backend Framework**: FastMCP (Model Context Protocol implementation)\n- **Runtime**: Python 3.11 with uvicorn ASGI server\n- **Transport**: Server-Sent Events (SSE) for real-time communication\n- **Data Storage**: In-memory storage with JSON file loading\n- **Deployment**: Single-process server running on port 8000\n\n## Key Components\n\n### 1. MCP Server (`main.py`)\n- **Purpose**: Main application entry point implementing MCP protocol\n- **Key Functions**:\n  - `load_sample_data()`: Loads documents from JSON file into memory\n  - `create_server()`: Configures FastMCP server with tools\n- **Architecture Decision**: Uses FastMCP library for simplified MCP implementation\n- **Rationale**: FastMCP abstracts away protocol complexity while maintaining full MCP compatibility\n\n### 2. Sample Data (`sample_data.json`)\n- **Purpose**: Static knowledge base with sample documents\n- **Structure**: Array of document records with metadata\n- **Fields**: id, title, text, content, url, metadata (category, author, date, tags)\n- **Architecture Decision**: JSON file for simple data storage\n- **Rationale**: Demonstrates data structure without database complexity\n\n### 3. Configuration Files\n- **`.replit`**: Defines Python 3.11 environment and workflow automation\n- **`pyproject.toml`**: Python project dependencies (fastmcp, uvicorn)\n- **`uv.lock`**: Dependency lock file for reproducible builds\n\n## Data Flow\n\n1. **Server Startup**: \n   - Load sample data from JSON file into memory\n   - Create lookup dictionary for fast document retrieval\n   - Initialize FastMCP server with search and fetch tools\n\n2. **Search Operations**:\n   - Accept keyword-based search queries\n   - Search across document titles, content, and metadata\n   - Return matching document summaries\n\n3. **Fetch Operations**:\n   - Accept document ID requests\n   - Return complete document with full content and metadata\n   - Provide fast retrieval through in-memory lookup\n\n4. **MCP Communication**:\n   - Use Server-Sent Events for real-time communication\n   - Follow MCP protocol for tool invocation and responses\n\n## External Dependencies\n\n### Core Dependencies\n- **fastmcp (>=2.9.0)**: MCP protocol implementation\n- **uvicorn (>=0.34.3)**: ASGI server for hosting\n- **pydantic**: Data validation (indirect dependency)\n\n### Development Environment\n- **Python 3.11**: Runtime environment\n- **Nix**: Package management and environment reproducibility\n\n## Deployment Strategy\n\n### Development Deployment\n- **Platform**: Replit environment with Nix package management\n- **Process**: Single uvicorn server process\n- **Port**: 8000 (configured in workflow)\n- **Auto-restart**: Enabled through Replit workflow configuration\n\n### Production Considerations\n- **Scaling**: Currently single-process, would need load balancing for production\n- **Data Persistence**: JSON file storage suitable for read-only scenarios\n- **Security**: No authentication implemented (would need to add for production)\n\n### Architecture Decision: In-Memory Storage\n- **Problem**: Need fast document retrieval for MCP operations\n- **Solution**: Load all documents into memory at startup\n- **Pros**: Fast lookup times, simple implementation\n- **Cons**: Limited by available memory, data lost on restart\n- **Alternative Considered**: Database storage (would add complexity for sample)\n\n## Recent Changes\n\n- **June 24, 2025**: Fixed deployment port configuration and URL formatting\n  - Updated server port from 5000 back to 8000 to match deployment requirements\n  - Reconfigured workflow to expect port 8000 instead of 5000\n  - Resolved deployment failure caused by port forwarding mismatch\n  - Fixed search function URL formatting to match fetch function format\n  - Both search and fetch now return proper OpenAI platform URLs for citations\n  - Server now properly configured for Autoscale deployments on port 8000\n  - MCP server successfully running and ready for deployment\n\n## Changelog\n\n```\nChangelog:\n- June 24, 2025: Complete MCP server with vector store integration\n  - FastMCP server with search/fetch tools\n  - OpenAI Vector Store semantic search integration\n  - OpenAI Vector Store file content retrieval\n  - Pure OpenAI API implementation (no fallbacks)\n  - SSE transport enabled for ChatGPT integration\n  - Comprehensive documentation and setup guide\n```\n\n## User Preferences\n\n```\nPreferred communication style: Simple, everyday language.\n```","size_bytes":4869},"validate_mcp.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nValidate MCP server functionality by testing the actual protocol methods\n\"\"\"\n\nimport asyncio\nimport main\n\nasync def validate_mcp_server():\n    \"\"\"Complete validation of MCP server functionality\"\"\"\n    \n    print(\"MCP Server Validation\")\n    print(\"=\" * 21)\n    \n    server = main.create_server()\n    \n    # Test 1: List tools\n    print(\"1. Testing list_tools...\")\n    try:\n        tools = await server._list_tools()\n        print(f\"   Tools found: {len(tools)}\")\n        \n        tool_data = []\n        for tool in tools:\n            data = {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"has_schema\": hasattr(tool, 'input_schema')\n            }\n            tool_data.append(data)\n            print(f\"   - {tool.name}: {tool.description[:50]}...\")\n        \n        # Verify expected tools\n        names = [t[\"name\"] for t in tool_data]\n        if \"search\" in names and \"fetch\" in names:\n            print(\"   ✓ Both required tools present\")\n        else:\n            print(f\"   ✗ Missing tools. Found: {names}\")\n            \n    except Exception as e:\n        print(f\"   ✗ List tools failed: {e}\")\n        return False\n    \n    # Test 2: Test search tool call simulation\n    print(\"\\n2. Testing search tool...\")\n    try:\n        # Simulate what happens when MCP client calls search\n        search_query = \"test search\"\n        \n        # Find search function and call it directly\n        search_functions = [f for name, f in server._tool_manager._tools.items() if name == \"search\"]\n        if search_functions:\n            # This simulates the MCP call\n            import main\n            if main.openai_client:\n                # Test the underlying OpenAI API call\n                response = main.openai_client.vector_stores.search(\n                    vector_store_id=main.VECTOR_STORE_ID,\n                    query=search_query\n                )\n                print(f\"   ✓ Search executed successfully, found {len(response.data)} results\")\n            else:\n                print(\"   ✗ OpenAI client not available\")\n        else:\n            print(\"   ✗ Search tool not found\")\n    except Exception as e:\n        print(f\"   ✗ Search test failed: {e}\")\n    \n    # Test 3: Test fetch tool call simulation\n    print(\"\\n3. Testing fetch tool...\")\n    try:\n        # Get a file ID from search results first\n        if main.openai_client:\n            search_response = main.openai_client.vector_stores.search(\n                vector_store_id=main.VECTOR_STORE_ID,\n                query=\"test\"\n            )\n            \n            if search_response.data:\n                file_id = search_response.data[0].file_id\n                \n                # Test fetch\n                content_response = main.openai_client.vector_stores.files.content(\n                    vector_store_id=main.VECTOR_STORE_ID,\n                    file_id=file_id\n                )\n                \n                if content_response.data:\n                    print(f\"   ✓ Fetch executed successfully, retrieved {len(content_response.data[0].text)} characters\")\n                else:\n                    print(\"   ✗ No content retrieved\")\n            else:\n                print(\"   ✗ No files to fetch\")\n        else:\n            print(\"   ✗ OpenAI client not available\")\n    except Exception as e:\n        print(f\"   ✗ Fetch test failed: {e}\")\n    \n    # Test 4: Server accessibility  \n    print(\"\\n4. Testing server accessibility...\")\n    try:\n        import httpx\n        async with httpx.AsyncClient(timeout=5.0) as client:\n            response = await client.get(\"http://0.0.0.0:8000/sse/\")\n            if response.status_code == 200:\n                print(\"   ✓ SSE endpoint accessible\")\n            else:\n                print(f\"   ✗ SSE endpoint returned {response.status_code}\")\n    except Exception as e:\n        print(f\"   ✗ Server not accessible: {e}\")\n    \n    print(\"\\nMCP validation completed!\")\n    return True\n\nif __name__ == \"__main__\":\n    asyncio.run(validate_mcp_server())","size_bytes":4080},"attached_assets/content-1750746448649.md":{"content":"","size_bytes":0}}}